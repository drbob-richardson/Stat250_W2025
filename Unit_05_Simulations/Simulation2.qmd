---
title: "Simulation: Simulating Data from a Probability Model"
subtitle: "BYU STAT 250"
author: "Dr. Scott D. Grimshaw"
format: 
  revealjs:
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    chalkboard: true
    center: false
    html-math-method: mathjax
execute:
  freeze: auto
---

```{r}
#| include: false
knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.818,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)
options(scipen = 100, width = 70)
library(tidyverse)
```

## Announcements


## Recap of last lecture

Simulation (Computing Random Variables)

Approximating ${\rm P}(A)$

Approximating ${\rm E}(Y)$


## Outline

Simulating Data from a Probability Model

Intro to Statistical Research using Simulation Studies

## 

### Connecting Probability Models with Data

Assume data is normally distributed

$Y_1,Y_2,\ldots,Y_n$ are a sample from a ${\rm N}(\mu,\sigma)$ distribution

If we were to get "another sample" what would it look like?


## Normally Distributed Data

`rnorm(n, mean, sd)`

```{r}
#| echo: false
#| eval: true
set.seed(2112)
n <- 50
simdata <- data.frame(type = factor(
                              c(rep("rnorm(n, mean = 0, sd = 1)", n),
                                rep("rnorm(n, mean = 1, sd = 1)", n),
                                rep("rnorm(n, mean = 0, sd = 2)", n)), 
                              levels = c("rnorm(n, mean = 0, sd = 1)", "rnorm(n, mean = 1, sd = 1)", "rnorm(n, mean = 0, sd = 2)")),
                      y = c(rnorm(n, mean = 0, sd = 1),
                            rnorm(n, mean = 1, sd = 1),
                            rnorm(n, mean = 0, sd = 2)) )
ggplot(simdata, aes(x = type, y = y)) +
  geom_boxplot() +
  labs(x = NULL, y = "Y")
```


## Other "Named" Distributions {.smaller}

::: panel-tabset
### Bernoulli(p)

::: {layout="[[1,1], [1,1]]" layout-valign="center"}
$$
{\scriptsize 
p(y) = \left\{ \begin{array}{ll}
               p & \mbox{if $y = 1$} \\
               1 - p & \mbox{if $y = 0$} \\
               0 & \mbox{otherwise}
        \end{array}\right.
}
$$

```{r}
#| echo: true
#| eval: true
simdata_Bern <- rbinom(n = 40, size = 1, p = 0.8)
head(simdata_Bern, 15)
```

```{r out.width = "80%"}
#| echo: false
#| eval: true
plot(c(0, 1), c(0.2, 0.8), pch = 19, 
     xlim = c(-0.25, 1.25), ylim = c(0, 1),
     xlab = "y", ylab = "p(y)")
lines(c(0, 0), c(0, 0.2), lty = 2)
lines(c(1, 1), c(0, 0.8), lty = 2)
```

```{r out.width = "60%"}
#| echo: false
#| eval: true
simdata <- data.frame(t = 1 : 40, Y = simdata_Bern)
ggplot(simdata, aes(x = Y)) +
  geom_bar(col = "black", fill = "white") 
```

:::

### Binomial(n, p)

::: {layout="[[1,1], [1,1]]" layout-valign="center"}
$$
{\scriptsize 
p(y) = \left\{ \begin{array}{ll}
               {n \choose y}\ p^y (1 - p)^{n - y} & \mbox{if $y = 0,1,\ldots,n$} \\
               0 & \mbox{otherwise}
        \end{array}\right.
}
$$

```{r}
#| echo: true
#| eval: true
simdata_Binom <- rbinom(n = 40, size = 12, p = 0.6)
head(simdata_Binom, 15)
```

```{r out.width = "80%"}
#| echo: false
#| eval: true
plot(0 : 12, dbinom(0 : 12, 12, 0.6), pch = 19, 
     xlim = c(-0.25, 12.25), ylim = c(0, 1),
     xlab = "y", ylab = "p(y)")
```

```{r out.width = "60%"}
#| echo: false
#| eval: true
simdata <- data.frame(t = 1 : 40, Y = simdata_Binom)
ggplot(simdata, aes(x = Y)) +
  geom_bar(col = "black", fill = "white") 
```

:::

### Exponential($\lambda$)

::: {layout="[[1,1], [1,1]]" layout-valign="center"}
$$
{\scriptsize 
f(y) = \left\{ \begin{array}{ll}
               \lambda e^{-\lambda y} & \mbox{if $y > 0$} \\
               0 & \mbox{otherwise}
        \end{array}\right.
}
$$


```{r}
#| echo: true
#| eval: true
simdata_Exp <- rexp(n = 40, rate = 4.5)
head(simdata_Exp, 4)
```

```{r out.width = "80%"}
#| echo: false
#| eval: true
plot(seq(0, 2, length = 200), dexp(seq(0, 2, length = 200), 4.5), type = "l",
     xlim = c(-0.25, 2.25), 
     xlab = "y", ylab = "f(y)")
abline(v = 0, lty = 2, col = "gray")
```

```{r out.width = "60%"}
#| echo: false
#| eval: true
simdata <- data.frame(t = 1 : 200, Y = simdata_Exp)
ggplot(simdata, aes(x = Y)) +
  geom_boxplot() 
```

:::



:::

## Other "Named" Distributions

Poisson

Uniform

Gamma

Lognormal





## AR(1) Model

$$
Y_t = \phi Y_{t-1} + \epsilon_t, \quad \epsilon_t \sim {\rm N}(0, \sigma_\epsilon) \quad i=2,\ldots,n
$$

with $Y_1 \sim {\rm N}(\mu, \sigma_\epsilon / \sqrt{1-\phi^2})$
where $|\phi| < 1$ and $\sigma_\epsilon > 0$

\

When $\phi$ is close to one there is high correlation between successive observations (we'll see "smoothness")

## AR(1) Model

::: panel-tabset
### Pseudo Code
```{r}
#| echo: true
#| eval: false
Assign values for n, phi, sigma_epsilon

Create simdata_AR1, an empty vector of length n

for(i in 1 : n){
  
  if FirstObs then 
    generate N(0, sigma_epsilon / sqrt(1 - phi^2))
    save value in simdata_AR1[1]
  
  for all other observations
    generate epsilon from N(0, sigma_epsilon)
    compute Yt = phi * Y_{t-1} + epsilon 
    save value in simdata_AR1[i]
  
}
```

### Code

Sample of $n = 100$ from an AR(1) with $\phi = 0.9$ and $\sigma_\epsilon = 1$ 

```{r}
#| echo: true
#| eval: false
n <- 100
phi <- 0.9
sigma_epsilon <- 1

simdata_AR1 <- rep(0, n)

for(i in 1 : n){
  if(i == 1){
    simdata_AR1[1] <- rnorm(1, mean = 0, sd = sigma_epsilon / sqrt(1 - phi^2))
  } else{
    simdata_AR1[i] <- phi * simdata_AR1[i - 1] + rnorm(1, mean = 0, sd = sigma_epsilon)
  }
}
```

### Data

```{r out.width = "50%"}
#| echo: false
#| eval: true
my_simAR1 <- function(n, rho, sigma_epsilon){
  simdata_AR1 <- rep(0, n)
  for(i in 1 : n){
    if(i == 1){
      simdata_AR1[1] <- rnorm(1, mean = 0, sd = sigma_epsilon / sqrt(1 - rho^2))
    } else{
      simdata_AR1[i] <- rho * simdata_AR1[i - 1] + rnorm(1, mean = 0, sd = sigma_epsilon)
    }
  }
  simdata_AR1
}

set.seed(2112)
simdata <- data.frame(t = 1 : 100, Y = my_simAR1(100, 0.9, 1))
ggplot(simdata, aes(x = t, y = Y)) +
  geom_point() +
  geom_line()
```

:::

## AR(1) Model

Compare $\phi$

::: panel-tabset
### $\phi = 0$

```{r out.width = "50%"}
#| echo: false
#| eval: true
#set.seed(2112)
simdata <- data.frame(t = 1 : 100, Y = my_simAR1(100, 0, 1))
ggplot(simdata, aes(x = t, y = Y)) +
  geom_point() +
  geom_line()
```

### $\phi = 0.2$

```{r out.width = "50%"}
#| echo: false
#| eval: true
#set.seed(2112)
simdata <- data.frame(t = 1 : 100, Y = my_simAR1(100, 0.2, 1))
ggplot(simdata, aes(x = t, y = Y)) +
  geom_point() +
  geom_line()
```

### $\phi = 0.6$

```{r out.width = "50%"}
#| echo: false
#| eval: true
set.seed(1756)
simdata <- data.frame(t = 1 : 100, Y = my_simAR1(100, 0.6, 1))
ggplot(simdata, aes(x = t, y = Y)) +
  geom_point() +
  geom_line()
```

### $\phi = 0.8$

```{r out.width = "50%"}
#| echo: false
#| eval: true
set.seed(2319)
simdata <- data.frame(t = 1 : 100, Y = my_simAR1(100, 0.8, 1))
ggplot(simdata, aes(x = t, y = Y)) +
  geom_point() +
  geom_line()
```

:::



## Data Modelling {.smaller}

Given data that has certain features, what is the probability model?

Time to Finish Watching a Show

::: {layout="[-5,25,-10]"}
![](images/BingeRacing.png)

:::


## Zero-Inflated Poisson {.smaller}

Y = Time to Finish Watching has two types of Viewers

::: columns
::: {.column width="40%"}
![](images/ZIPoissonGraphic.png)
:::

::: {.column width="60%"}

-   Binge Racers (watch all episodes in one day)

      P(Binge Racer) = 0.15

      P(Binge Watcher) = 0.85

-   Binge Watchers ~ Poisson(2.2)

```{r out.width = "50%"}
#| echo: false
#| eval: true
#| fig-align: left
plot(1 : 12, dpois(1 : 12, 2.2), pch = 19)
```

:::

:::

## Zero-Inflated Poisson {.smaller}

Y = Time to Finish Watching 

::: panel-tabset
### Pseudo Code: Loop
```{r}
#| echo: true
#| eval: false
Assign values for n, p, lambda

Create simdata_ZIPois, an empty vector of length n

for(i in 1 : n){

  generate X from Bernoulli(p) 
    
  if X is 1 (meaning it's a Binge Racer)
    simdata_ZIPois[i] = 0

  else (meaning it's a Binge Watcher)
    generate Y from Poisson(lambda)
    save value in simdata_ZIPois[i]
  
}

```

### Code: Loop

Sample of $n = 200$ from a Zero-Inflated Poisson with $p = 0.2$ and $\lambda = 2.5$ 

```{r}
#| echo: true
#| eval: false
n <- 200
p <- 0.15
lambda <- 2.2

simdata_ZIPois <- rep(0, n)

for(i in 1 : n){
  x <- rbinom(1, size = 1, prob = p)
  if(x == 1){
    simdata_ZIPois[i] <- 0
  } else{
    simdata_ZIPois[i] <- rpois(1, lambda = lambda)
  }
}
```

### Data

```{r out.width = "55%"}
#| echo: false
#| eval: true
set.seed(1756)

n <- 200
p <- 0.15
lambda <- 2.2

simdata_ZIPois <- rep(0, n)

for(i in 1 : n){
  x <- rbinom(1, size = 1, prob = p)
  if(x == 1){
    simdata_ZIPois[i] <- 0
  } else{
    simdata_ZIPois[i] <- rpois(1, lambda = lambda)
  }
}

simdata <- data.frame(t = 1 : n, Y = simdata_ZIPois)
ggplot(simdata, aes(x = Y)) +
  geom_histogram(breaks = seq(-0.5, 10.5, by = 1), col = "black", fill = "white") 
```

:::


## Zero-Inflated Poisson {.smaller}

Y = Time to Finish Watching 

::: panel-tabset
### Pseudo Code: Vectorized
```{r}
#| echo: true
#| eval: false
Assign values for n, p, lambda

generate n values of X from Bernoulli(p) 

generate n values of Y from Poisson(lambda)

Create simdata_ZIPois using ifelse
  if Xi is 1 (meaning it's a Binge Racer) then 0
  if Xi is 0 (meaning it's a Binge Watcher) then Y

```

### Code: Vectorized

Sample of $n = 200$ from a Zero-Inflated Poisson with $p = 0.2$ and $\lambda = 2.5$ 

```{r}
#| echo: true
#| eval: true
n <- 200
p <- 0.15
lambda <- 2.2

x <- rbinom(n, size = 1, prob = p)
head(x, 15)

y <- rpois(n, lambda = lambda)
head(y, 15)

simdata_ZIPois <- ifelse(x == 1, 0, y)
head(simdata_ZIPois, 15)
```

### Data

```{r out.width = "55%"}
#| echo: false
#| eval: true
set.seed(2319)

n <- 200
p <- 0.15
lambda <- 2.2

x <- rbinom(n, size = 1, prob = p)
#head(x)

y <- rpois(n, lambda = lambda)
#head(y)

simdata_ZIPois <- ifelse(x == 1, 0, y)
#head(simdata_ZIPois)

simdata <- data.frame(t = 1 : n, Y = simdata_ZIPois)
ggplot(simdata, aes(x = Y)) +
  geom_histogram(breaks = seq(-0.5, 10.5, by = 1), col = "black", fill = "white") 
```

:::

## Data Modelling 

Given data that has certain features, what is the probability model?

Yelp Review Summary

::: {layout="[-5,45,-10]"}
![](images/BombayHouseYelpReviews.png)
:::


## Latent Variable for Ordinal Data {.smaller}

Y = Number of Stars in Review

-   Latent (Unobserved) Measurement ~ Normal(4.7, 0.6)

-   Ordinal Discretized Measurement

```{r}
#| echo: false
#| eval: true
z <- seq(0.5, 6.5, length = 300)
plot(z, dnorm(z, mean = 4.7, sd = 0.6), type = "l", xlab = "X", ylab = "Density")
abline(v = (1 : 4) + 0.5)
text(1 : 5, rep(0.15, 5), 1 : 5, cex = 1.3, col = "royalblue")
```

## Latent Variable for Ordinal Data {.smaller}

Y = Number of Stars in Review

::: panel-tabset
### Pseudo Code: Vectorized
```{r}
#| echo: true
#| eval: false
Assign values for n, mu, sigma_epsilon

generate n values of X from N(mu, sigma)

Create simdata_DiscNorm by rounding to 1, 2, 3, 4, 5
  note: trim Xi values to 1 and 5
```

### Code: Vectorized

Sample of $n = 930$ from Ordinal Data Model from N(4.7, 0.6)

```{r}
#| echo: true
#| eval: true
n <- 930
mu <- 4.7
sigma <- 0.6

x <- rnorm(n, mean = mu, sd = sigma)
head(x, 15)

simdata_DiscNorm <- round(x)
simdata_DiscNorm <- ifelse(simdata_DiscNorm < 1, 1, simdata_DiscNorm)
simdata_DiscNorm <- ifelse(simdata_DiscNorm > 5, 5, simdata_DiscNorm)

head(simdata_DiscNorm, 15)
```

### Data

```{r out.width = "55%"}
#| echo: false
#| eval: true
set.seed(2319)

n <- 930
mu <- 4.7
sigma <- 0.6

x <- rnorm(n, mean = mu, sd = sigma)
#head(x, 15)

simdata_DiscNorm <- round(x)
simdata_DiscNorm <- ifelse(simdata_DiscNorm < 1, 1, simdata_DiscNorm)
simdata_DiscNorm <- ifelse(simdata_DiscNorm > 5, 5, simdata_DiscNorm)

#head(simdata_DiscNorm, 15)

simdata <- data.frame(t = 1 : n, Y = simdata_DiscNorm)
ggplot(simdata, aes(x = Y)) +
  geom_bar(col = "black", fill = "white") +
  xlim(0.5, 5.5)
```

:::

## Intro to Statistical Research using Simulation Studies


How large $n$ to identify normality from histogram?

Thinking:

-   generate a sample from N(0, 1)

-   create histogram

Note: start with small $n$ and increase until reliably normal

## Sample Size Needed to Identify Normal from Histogram

::: panel-tabset
### Code
```{r}
#| echo: true
#| eval: true
IsItNormal <- function(n){
  # generate data we know are normal
  # note: N(0, 1) WLOG
  simdata_Normal <- rnorm(n, mean = 0, sd = 1)
  # visualize with histogram
  simdata <- data.frame(simdata_Normal = simdata_Normal)
  ggplot(simdata, aes(x = simdata_Normal)) +
    geom_histogram(aes(y = after_stat(density))) +
    stat_function(fun = dnorm,
                  args = list(mean = mean(simdata$simdata_Normal),
                              sd = sd(simdata$simdata_Normal)),
                  col = "royalblue") +
    labs(x = "Y")
}
```

### n = 30

```{r out.width = "45%"}
#| echo: true
#| eval: true
IsItNormal(n = 30)
```

### Your Turn

-   Repeat `IsItNormal(n = ???)` changing the sample size until you feel you recognize the normal from the histogram

-   Confirm your choice with repeated runs of the same $n$

-   How does this simulation study inform your statistical practice?

### Stretch Exercise

Modify the code to produce the density curve (`geom_density`)

Do you result in a smaller $n$? How does this simulation study inform your statistical practice?

\ 

Modify the code to produce the QQ-Plot (`geom_qq` and `geom_qqline`)

:::



## Lecture Review

Simulating Data from a Probability Model

Intro to Statistical Research using Simulation Studies
